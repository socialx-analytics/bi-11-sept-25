{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXr7zlUpHEhaLbL6Knfc0E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/socialx-analytics/bi-11-sept-25/blob/main/007_unstructured_data_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Section 1: Setup and Load Data**"
      ],
      "metadata": {
        "id": "3KvnKtQ_WlLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "packages = [\n",
        "    \"transformers\",\n",
        "    \"scikit-learn\",\n",
        "    \"plotly\",\n",
        "    \"bertopic\",\n",
        "    \"sentence-transformers\",\n",
        "    \"umap-learn\",\n",
        "    \"hdbscan\",\n",
        "    \"wordcloud\"\n",
        "]\n",
        "\n",
        "for package in packages:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])"
      ],
      "metadata": {
        "id": "G5Lxn7U8Wr5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "# For sentiment analysis\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# For clustering\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.manifold import MDS\n",
        "import plotly.graph_objects as go\n",
        "import plotly.colors as pcolors\n",
        "\n",
        "# For topic modeling\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance\n",
        "\n",
        "# Import community detection\n",
        "from networkx.algorithms.community import greedy_modularity_communities"
      ],
      "metadata": {
        "id": "eiApsEDiWvFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import Data**"
      ],
      "metadata": {
        "id": "TCU-J86gW3Pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the CSV file\n",
        "df = pd.read_csv(\"tweet_digitalcurrency_text.csv\").head(1000)\n",
        "\n",
        "# Display the first 5 rows\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "n1gB7RGEW0MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display column names\n",
        "df.columns.tolist()"
      ],
      "metadata": {
        "id": "aw_DyMm3W7W9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display dataset info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "ESzOqMulW-bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Section 2: Social Network Analysis (SNA)**"
      ],
      "metadata": {
        "id": "sd31GZ6pXFkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create Edgelist from Twitter Mentions**"
      ],
      "metadata": {
        "id": "mopb2OsoXKYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create empty list to store edges\n",
        "edge_list = []\n",
        "\n",
        "# Loop through each tweet in the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    try:\n",
        "        # Get the username of the tweet author\n",
        "        account_name = row[\"screen_name\"]\n",
        "\n",
        "        # Get the tweet text\n",
        "        tweet = row[\"text\"]\n",
        "\n",
        "        # Convert tweet to string to handle any data type issues\n",
        "        tweet = str(tweet)\n",
        "\n",
        "        # Find all mentions in the tweet (usernames starting with @)\n",
        "        mentions = re.findall(r\"@(\\w+)\", tweet)\n",
        "\n",
        "        # Create edges between the author and each mentioned user\n",
        "        # Add @ symbol to both source and target\n",
        "        edges = [(f\"@{account_name}\", f\"@{mention}\") for mention in mentions]\n",
        "\n",
        "        # Add these edges to our edge list\n",
        "        edge_list.extend(edges)\n",
        "    except Exception as e:\n",
        "        continue\n",
        "\n",
        "# Convert edge list to DataFrame with source and target columns\n",
        "edgelist = pd.DataFrame(edge_list, columns=[\"source\", \"target\"])\n",
        "\n",
        "# Display the edgelist\n",
        "edgelist.head(10)"
      ],
      "metadata": {
        "id": "cVIId7dFXCw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total edges found: {len(edgelist)}\")\n",
        "edgelist"
      ],
      "metadata": {
        "id": "J3DwNIx1XUN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Network Construction and Visualization**"
      ],
      "metadata": {
        "id": "OLicDw1WXZhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct Network from edgelist\n",
        "G = nx.from_pandas_edgelist(edgelist, source=\"source\", target=\"target\")\n",
        "\n",
        "# Create visualization\n",
        "plt.figure(figsize=(30, 30))\n",
        "\n",
        "# Visualize Network\n",
        "nx.draw(\n",
        "    G,\n",
        "    with_labels=True,\n",
        "    node_color=\"skyblue\",\n",
        "    node_size=1200,\n",
        "    arrowstyle=\"->\",\n",
        "    arrowsize=20,\n",
        "    edge_color=\"r\",\n",
        "    font_size=9,\n",
        "    pos=nx.kamada_kawai_layout(G),\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8UADI1tjXhLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Network Metrics and Measurement**"
      ],
      "metadata": {
        "id": "P7MTOqZeXkWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Network Topology Measurement**"
      ],
      "metadata": {
        "id": "lofsQbPqXmKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of Nodes: {nx.number_of_nodes(G)}\")\n",
        "print(f\"Number of Edges: {nx.number_of_edges(G)}\")\n",
        "print(f\"Graph Density: {nx.density(G):.4f}\")"
      ],
      "metadata": {
        "id": "RVX4IMaHXypH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Centrality Measurements**"
      ],
      "metadata": {
        "id": "tCVpYGQyX4Fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Degree Centrality\n",
        "degree = nx.degree_centrality(G)\n",
        "\n",
        "# Top 10 nodes by Degree\n",
        "sorted_degree = sorted(nx.degree(G), key=lambda x: x[1], reverse=True)[0:10]\n",
        "for node, deg in sorted_degree:\n",
        "    print(f\"  {node}: {deg}\")"
      ],
      "metadata": {
        "id": "dAZu-0RYX1ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Betweenness Centrality\n",
        "betweenness = nx.betweenness_centrality(G)\n",
        "\n",
        "# Top 10 nodes by Betweenness Centrality\n",
        "sorted_betweenness = sorted(\n",
        "    nx.betweenness_centrality(G, normalized=True).items(),\n",
        "    key=lambda x: x[1],\n",
        "    reverse=True,\n",
        ")[0:10]\n",
        "for node, score in sorted_betweenness:\n",
        "    print(f\"  {node}: {score:.4f}\")"
      ],
      "metadata": {
        "id": "pVbiZCz3X-Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Closeness Centrality\n",
        "closeness = nx.closeness_centrality(G)\n",
        "\n",
        "# Top 10 nodes by Closeness Centrality\n",
        "sorted_closeness = sorted(\n",
        "    nx.closeness_centrality(G).items(), key=lambda x: x[1], reverse=True\n",
        ")[0:10]\n",
        "for node, score in sorted_closeness:\n",
        "    print(f\"  {node}: {score:.4f}\")"
      ],
      "metadata": {
        "id": "15drZ1i-YA_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize network based on degree\n",
        "plt.figure(figsize=(30, 30))\n",
        "\n",
        "# Set Degree Dictionary\n",
        "d = dict(degree)\n",
        "\n",
        "# Convert dict_keys to list for nodelist\n",
        "nodelist = list(d.keys())\n",
        "\n",
        "# Visualize Network based on degree\n",
        "nx.draw(\n",
        "    G,\n",
        "    with_labels=True,\n",
        "    node_color=\"skyblue\",\n",
        "    nodelist=nodelist,  # Use the list instead of dict_keys\n",
        "    node_size=[v * 90000 for v in d.values()],\n",
        "    arrowstyle=\"->\",\n",
        "    arrowsize=20,\n",
        "    edge_color=\"r\",\n",
        "    font_size=10,\n",
        "    pos=nx.kamada_kawai_layout(G),\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JicvjfKCYF-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Community Detection**"
      ],
      "metadata": {
        "id": "yrrrsnv1YMVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use greedy modularity communities for community detection\n",
        "communities_m = sorted(greedy_modularity_communities(G), key=len, reverse=True)\n",
        "print(f\"Number of communities found: {len(communities_m)}\")\n",
        "\n",
        "# Set Node Community Function\n",
        "def set_node_community(G, communities_m):\n",
        "    \"\"\"Add community to node attributes\"\"\"\n",
        "    for c, v_c in enumerate(communities_m):\n",
        "        for v in v_c:\n",
        "            # Add 1 to save 0 for external edges\n",
        "            G.nodes[v][\"community\"] = c + 1\n",
        "\n",
        "# Set Colour Function\n",
        "def get_color(i, r_off=1, g_off=1, b_off=1):\n",
        "    \"\"\"Assign a color to a vertex.\"\"\"\n",
        "    n = 16\n",
        "    low, high = 0.1, 0.9\n",
        "    span = high - low\n",
        "    r = low + span * (((i + r_off) * 3) % n) / (n - 1)\n",
        "    g = low + span * (((i + g_off) * 5) % n) / (n - 1)\n",
        "    b = low + span * (((i + b_off) * 7) % n) / (n - 1)\n",
        "    return (r, g, b)\n",
        "\n",
        "# Set Node Communities\n",
        "community = set_node_community(G, communities_m)\n",
        "\n",
        "# Set Node Color\n",
        "node_color = [get_color(G.nodes[v][\"community\"]) for v in G.nodes]\n",
        "\n",
        "# Visualize Network based on community\n",
        "plt.figure(figsize=(30, 30))\n",
        "nx.draw(\n",
        "    G,\n",
        "    with_labels=True,\n",
        "    node_color=node_color,\n",
        "    node_size=1200,\n",
        "    arrowstyle=\"->\",\n",
        "    arrowsize=20,\n",
        "    edge_color=\"r\",\n",
        "    font_size=10,\n",
        "    pos=nx.kamada_kawai_layout(G),\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o4ExPYk7YL6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Section 3: Text Mining**"
      ],
      "metadata": {
        "id": "JtID2-qDYUlh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Word Cloud**"
      ],
      "metadata": {
        "id": "LyygfxcbYcVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text column from dataframe\n",
        "tweet_texts = df[\"text\"]\n",
        "\n",
        "# Preprocessing for wordcloud\n",
        "# Remove URLs\n",
        "tweet_texts_clean = tweet_texts.str.replace(r'http\\S+|www.\\S+', '', regex=True)\n",
        "\n",
        "# Remove mentions and hashtags (optional, comment out if you want to keep them)\n",
        "tweet_texts_clean = tweet_texts_clean.str.replace(r'@\\w+|#\\w+', '', regex=True)\n",
        "\n",
        "# Custom stopwords for digital currency context\n",
        "custom_stopwords = set([\n",
        "    'digital', 'currency', 'crypto', 'bitcoin', 'btc',\n",
        "    'cryptocurrency', 'blockchain', 'coin', 'token',\n",
        "    'rt', 'amp', 'https', 'http', 'www', 'com'\n",
        "])\n",
        "\n",
        "# Combine all tweets into one large text\n",
        "all_tweets = \" \".join(tweet_texts_clean.astype(str))\n",
        "\n",
        "# Generate word cloud with custom stopwords\n",
        "stopwords = STOPWORDS.union(custom_stopwords)\n",
        "\n",
        "wordcloud = WordCloud(\n",
        "    width=800,\n",
        "    height=400,\n",
        "    background_color=\"white\",\n",
        "    colormap=\"viridis\",\n",
        "    max_words=100,\n",
        "    stopwords=stopwords\n",
        ").generate(all_tweets)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Word Cloud of Digital Currency Tweets\", fontsize=20)\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rdCxs0MqYeel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text Clustering with K-Means**"
      ],
      "metadata": {
        "id": "BzUFkMD9ZNWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use cleaned text from sentiment analysis\n",
        "clustering_texts = df[\"text\"].str.lower()\n",
        "\n",
        "# Clean text - remove URLs and mentions\n",
        "clustering_texts = clustering_texts.str.replace(r'http\\S+|www.\\S+', '', regex=True)\n",
        "clustering_texts = clustering_texts.str.replace(r'@\\w+', '', regex=True)\n",
        "clustering_texts = clustering_texts.dropna()\n",
        "clustering_texts = clustering_texts.reset_index(drop=True)\n",
        "\n",
        "print(f\"Total texts for clustering: {len(clustering_texts)}\")"
      ],
      "metadata": {
        "id": "dTsfxsyJZJXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate TF-IDF embeddings\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=1000,\n",
        "    stop_words='english',\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=2,\n",
        "    max_df=0.95\n",
        ")\n",
        "tfidf_matrix = vectorizer.fit_transform(clustering_texts).toarray()"
      ],
      "metadata": {
        "id": "cQmhrlmTZTgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find optimal number of clusters using silhouette score\n",
        "silhouette_scores = []\n",
        "K = range(2, min(10, len(tfidf_matrix)))\n",
        "\n",
        "for k in K:\n",
        "    kmeans = KMeans(n_clusters=k, init=\"k-means++\", random_state=42)\n",
        "    kmeans.fit(tfidf_matrix)\n",
        "    score = silhouette_score(tfidf_matrix, kmeans.labels_)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "optimal_k = K[np.argmax(silhouette_scores)]\n",
        "print(f\"Optimal number of clusters: {optimal_k}\")"
      ],
      "metadata": {
        "id": "KMZSAG2QZXSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform clustering with optimal k\n",
        "kmeans = KMeans(n_clusters=optimal_k, init=\"k-means++\", random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(tfidf_matrix)\n",
        "\n",
        "# Add cluster labels to dataframe\n",
        "df_clustered = df.copy()\n",
        "df_clustered[\"cluster\"] = cluster_labels\n",
        "\n",
        "# Get cluster distribution\n",
        "cluster_counts = df_clustered['cluster'].value_counts().sort_index()\n",
        "for cluster, count in cluster_counts.items():\n",
        "    percentage = (count / len(df_clustered)) * 100\n",
        "    print(f\"Cluster {cluster}: {count} tweets ({percentage:.1f}%)\")"
      ],
      "metadata": {
        "id": "zYNrKdL4ZaH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MDS for 2D visualization\n",
        "mds = MDS(n_components=2, random_state=42, n_init=4)\n",
        "vis_dims = mds.fit_transform(tfidf_matrix)\n",
        "\n",
        "# Create visualization dataframe\n",
        "df_vis = df_clustered.copy()\n",
        "df_vis[\"mds_x\"] = vis_dims[:, 0]\n",
        "df_vis[\"mds_y\"] = vis_dims[:, 1]"
      ],
      "metadata": {
        "id": "V871CLdYZc-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create interactive plotly visualization\n",
        "fig = go.Figure()\n",
        "colors = pcolors.qualitative.Plotly\n",
        "colors = colors * (optimal_k // len(colors) + 1)\n",
        "\n",
        "for cluster in range(optimal_k):\n",
        "    cluster_data = df_vis[df_vis[\"cluster\"] == cluster]\n",
        "\n",
        "    if len(cluster_data) == 0:\n",
        "        continue\n",
        "\n",
        "    cluster_color = colors[cluster]\n",
        "\n",
        "    # Get sample texts for hover\n",
        "    sample_texts = cluster_data[\"text\"].str[:100] + \"...\"\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=cluster_data[\"mds_x\"],\n",
        "            y=cluster_data[\"mds_y\"],\n",
        "            mode=\"markers\",\n",
        "            name=f\"Cluster {cluster}\",\n",
        "            marker=dict(\n",
        "                color=cluster_color,\n",
        "                size=8,\n",
        "                opacity=0.7,\n",
        "            ),\n",
        "            text=sample_texts,\n",
        "            hovertemplate=(\n",
        "                f\"<b>Cluster {cluster}</b><br>\"\n",
        "                \"Text: %{text}<extra></extra>\"\n",
        "            ),\n",
        "        )\n",
        "    )\n",
        "\n",
        "fig.update_layout(\n",
        "    title=dict(\n",
        "        text=f\"Text Clustering Visualization (K={optimal_k})\",\n",
        "        x=0.5,\n",
        "        xanchor=\"center\",\n",
        "    ),\n",
        "    xaxis_title=\"MDS Component 1\",\n",
        "    yaxis_title=\"MDS Component 2\",\n",
        "    height=600,\n",
        "    showlegend=True,\n",
        "    hovermode='closest'\n",
        ")\n",
        "\n",
        "# Save interactive plot\n",
        "fig.write_html(\"clustering_visualization.html\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "9Y2n2ud3ZiIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display static plot using matplotlib\n",
        "plt.figure(figsize=(12, 8))\n",
        "scatter = plt.scatter(df_vis[\"mds_x\"], df_vis[\"mds_y\"],\n",
        "                     c=df_vis[\"cluster\"],\n",
        "                     cmap='viridis',\n",
        "                     alpha=0.6,\n",
        "                     s=50)\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.xlabel('MDS Component 1')\n",
        "plt.ylabel('MDS Component 2')\n",
        "plt.title(f'Text Clustering Visualization (K={optimal_k})')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RGFTHztRZljs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show top terms for each cluster\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "for cluster_id in range(optimal_k):\n",
        "    # Get texts in this cluster\n",
        "    cluster_mask = cluster_labels == cluster_id\n",
        "    cluster_tfidf = tfidf_matrix[cluster_mask]\n",
        "\n",
        "    # Calculate mean TF-IDF scores for this cluster\n",
        "    if len(cluster_tfidf) > 0:\n",
        "        mean_tfidf = cluster_tfidf.mean(axis=0)\n",
        "        top_indices = mean_tfidf.argsort()[-10:][::-1]\n",
        "        top_terms = [feature_names[i] for i in top_indices]\n",
        "\n",
        "        print(f\"\\nCluster {cluster_id}: {top_terms[:5]}\")"
      ],
      "metadata": {
        "id": "Tob50rizZp0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get sample tweets from each cluster\n",
        "for cluster_id in range(optimal_k):\n",
        "    cluster_tweets = df_clustered[df_clustered['cluster'] == cluster_id]['text'].head(2)\n",
        "    print(f\"\\nCluster {cluster_id}:\")\n",
        "    for i, tweet in enumerate(cluster_tweets, 1):\n",
        "        print(f\"  {i}. {tweet[:100]}...\")"
      ],
      "metadata": {
        "id": "apFLaaiRZtTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save clustered data\n",
        "output_clustered = \"tweet_digitalcurrency_clustered.csv\"\n",
        "df_clustered.to_csv(output_clustered, index=False)"
      ],
      "metadata": {
        "id": "gq70WR-ZZzVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sentiment Analysis with BERT**"
      ],
      "metadata": {
        "id": "cHFYb9-tYvWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Pretrained Model for sentiment\n",
        "pretrained = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "\n",
        "# Set Model and Tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(pretrained)\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained)\n",
        "\n",
        "# Set Pipeline\n",
        "sentiment_analysis = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Function to clean text for sentiment analysis\n",
        "def clean_text_for_sentiment(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "    return text[:512]  # Limit to 512 characters for BERT\n",
        "\n",
        "# Function to perform sentiment analysis\n",
        "def get_sentiment(text):\n",
        "    try:\n",
        "        if pd.isna(text) or text.strip() == '':\n",
        "            return 'neutral', 0.0\n",
        "\n",
        "        cleaned_text = clean_text_for_sentiment(str(text))\n",
        "        result = sentiment_analysis(cleaned_text)[0]\n",
        "\n",
        "        # Map labels to consistent format\n",
        "        label_map = {\n",
        "            'LABEL_0': 'negative',\n",
        "            'LABEL_1': 'neutral',\n",
        "            'LABEL_2': 'positive',\n",
        "            'NEGATIVE': 'negative',\n",
        "            'NEUTRAL': 'neutral',\n",
        "            'POSITIVE': 'positive'\n",
        "        }\n",
        "\n",
        "        sentiment = label_map.get(result['label'], result['label'].lower())\n",
        "        score = result['score']\n",
        "\n",
        "        return sentiment, score\n",
        "    except:\n",
        "        return 'neutral', 0.0"
      ],
      "metadata": {
        "id": "iMIkhvFAYjHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply sentiment analysis to all tweets\n",
        "sentiments = []\n",
        "scores = []\n",
        "\n",
        "for idx, text in enumerate(df['text']):\n",
        "    if idx % 100 == 0:\n",
        "        print(f\"Processing tweet {idx}/{len(df)}...\")\n",
        "\n",
        "    sentiment, score = get_sentiment(text)\n",
        "    sentiments.append(sentiment)\n",
        "    scores.append(score)\n",
        "\n",
        "# Add sentiment results to dataframe\n",
        "df['sentiment'] = sentiments\n",
        "df['sentiment_score'] = scores\n",
        "\n",
        "# Display sample results\n",
        "df[['text', 'sentiment', 'sentiment_score']].head(10)"
      ],
      "metadata": {
        "id": "M4bcOwuwY21o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment Distribution\n",
        "sentiment_counts = df['sentiment'].value_counts()\n",
        "sentiment_counts"
      ],
      "metadata": {
        "id": "CjIfT5PjY7sK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create bar chart visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = {'positive': 'green', 'neutral': 'gray', 'negative': 'red'}\n",
        "sentiment_counts.plot(kind='bar', color=[colors.get(x, 'blue') for x in sentiment_counts.index])\n",
        "plt.title('Sentiment Distribution of Digital Currency Tweets', fontsize=16)\n",
        "plt.xlabel('Sentiment', fontsize=12)\n",
        "plt.ylabel('Number of Tweets', fontsize=12)\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, v in enumerate(sentiment_counts):\n",
        "    plt.text(i, v + 50, str(v), ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate percentage distribution\n",
        "for sentiment, count in sentiment_counts.items():\n",
        "    percentage = (count / len(df)) * 100\n",
        "    print(f\"{sentiment}: {count} tweets ({percentage:.1f}%)\")"
      ],
      "metadata": {
        "id": "deAhDXpCY_BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results to CSV\n",
        "output_filename = \"tweet_digitalcurrency_with_sentiment.csv\"\n",
        "df.to_csv(output_filename, index=False)"
      ],
      "metadata": {
        "id": "U28mQjtSZC3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Topic Modeling with BERTopic**"
      ],
      "metadata": {
        "id": "f0FGJeVwZ5l9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the text column and clean it\n",
        "input_docs = df[\"text\"].str.lower()\n",
        "\n",
        "# Remove URLs and clean text\n",
        "input_docs = input_docs.str.replace(r'http\\S+|www.\\S+', '', regex=True)\n",
        "input_docs = input_docs.str.replace(r'@\\w+', '', regex=True)  # Remove mentions\n",
        "input_docs = input_docs.dropna()\n",
        "input_docs = input_docs.reset_index(drop=True)\n",
        "\n",
        "print(f\"Total documents for topic modeling: {len(input_docs)}\")"
      ],
      "metadata": {
        "id": "jRMZaw44aAj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create embeddings using multilingual model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Faster English model\n",
        "embeddings = embedding_model.encode(input_docs.tolist(), show_progress_bar=True)"
      ],
      "metadata": {
        "id": "f9ApaiwqaBcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up UMAP for dimensionality reduction\n",
        "umap_model = UMAP(\n",
        "    n_neighbors=15,\n",
        "    n_components=5,\n",
        "    min_dist=0.0,\n",
        "    metric='cosine',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Set up HDBSCAN for clustering\n",
        "hdbscan_model = HDBSCAN(\n",
        "    min_cluster_size=30,  # Adjust based on dataset size\n",
        "    metric='euclidean',\n",
        "    cluster_selection_method='eom',\n",
        "    prediction_data=True\n",
        ")\n",
        "\n",
        "# Pre-reduce embeddings for visualization\n",
        "reduced_embeddings = UMAP(\n",
        "    n_neighbors=15,\n",
        "    n_components=2,\n",
        "    min_dist=0.0,\n",
        "    metric='cosine',\n",
        "    random_state=42\n",
        ").fit_transform(embeddings)\n",
        "\n",
        "# Representation models for better topic descriptions\n",
        "keybert = KeyBERTInspired()\n",
        "mmr = MaximalMarginalRelevance(diversity=0.3)\n",
        "representation_model = {\n",
        "    \"KeyBERT\": keybert,\n",
        "    \"MMR\": mmr,\n",
        "}"
      ],
      "metadata": {
        "id": "0hhyLVONaFPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and train BERTopic model\n",
        "topic_model = BERTopic(\n",
        "    embedding_model=embedding_model,\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    representation_model=representation_model,\n",
        "    top_n_words=10,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Train model\n",
        "topics, probs = topic_model.fit_transform(input_docs, embeddings)"
      ],
      "metadata": {
        "id": "xXF70p0uaJWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get topic information\n",
        "topic_info = topic_model.get_topic_info()\n",
        "print(f\"Number of topics found: {len(topic_info) - 1}\")  # Exclude noise topic (-1)\n",
        "\n",
        "# Display top topics\n",
        "topic_info[topic_info.Topic != -1].head(10)[['Topic', 'Count', 'Name']]"
      ],
      "metadata": {
        "id": "5If4GH2AaNsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get topic distribution\n",
        "def get_topic_distribution(topic_model, input_docs):\n",
        "    # Get document info\n",
        "    doc_info = topic_model.get_document_info(input_docs)\n",
        "\n",
        "    # Count documents per topic\n",
        "    topic_counts = doc_info['Topic'].value_counts().sort_index()\n",
        "\n",
        "    # Total documents\n",
        "    total_docs = len(doc_info)\n",
        "    total_docs_wo_noise = len(doc_info[doc_info['Topic'] != -1])\n",
        "\n",
        "    # Get keywords for each topic\n",
        "    topic_keywords = doc_info.groupby('Topic')['Top_n_words'].first()\n",
        "\n",
        "    # Create summary\n",
        "    summary = pd.DataFrame({\n",
        "        'Topic': topic_counts.index,\n",
        "        'n_documents': topic_counts.values,\n",
        "        'percentage': (topic_counts.values / total_docs) * 100,\n",
        "        'keywords': topic_keywords.values\n",
        "    })\n",
        "\n",
        "    # Calculate percentage without noise\n",
        "    summary['percentage_wo_noise'] = summary.apply(\n",
        "        lambda row: (row['n_documents'] / total_docs_wo_noise * 100) if row['Topic'] != -1 else 0,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Get topic distribution\n",
        "topic_summary = get_topic_distribution(topic_model, input_docs)\n",
        "\n",
        "# Display topic summary\n",
        "topic_summary[topic_summary['Topic'] != -1].head(10)\n"
      ],
      "metadata": {
        "id": "rcedDAfvaRNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save topic summary\n",
        "topic_summary.to_csv('topic_summary.csv', index=False)"
      ],
      "metadata": {
        "id": "P5C-va-EaX2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize topic distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "topic_counts = topic_summary[topic_summary['Topic'] != -1]['n_documents']\n",
        "topic_labels = [f\"Topic {i}\" for i in topic_summary[topic_summary['Topic'] != -1]['Topic']]\n",
        "\n",
        "plt.bar(range(len(topic_counts)), topic_counts)\n",
        "plt.xlabel('Topic')\n",
        "plt.ylabel('Number of Documents')\n",
        "plt.title('Distribution of Documents across Topics')\n",
        "plt.xticks(range(len(topic_counts)), topic_labels, rotation=45, ha='right')\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, v in enumerate(topic_counts):\n",
        "    plt.text(i, v + 10, str(v), ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nq2tIPqraYNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and save topic visualization\n",
        "fig = topic_model.visualize_documents(\n",
        "    input_docs,\n",
        "    reduced_embeddings=reduced_embeddings,\n",
        "    hide_annotations=True,\n",
        "    hide_document_hover=False,\n",
        "    custom_labels=True\n",
        ")\n",
        "\n",
        "# Save visualization\n",
        "fig.write_html(\"topic_visualization.html\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "ipqjsdSWaboZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get most representative documents for top topics\n",
        "for topic in topic_summary[topic_summary['Topic'] != -1]['Topic'].head(5):\n",
        "    print(f\"\\nTopic {topic}:\")\n",
        "    representative_docs = topic_model.get_representative_docs(topic)\n",
        "    for i, doc in enumerate(representative_docs[:2]):  # Show 2 examples per topic\n",
        "        print(f\"  {i+1}. {doc[:100]}...\")"
      ],
      "metadata": {
        "id": "F-MlU2pNafBE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}