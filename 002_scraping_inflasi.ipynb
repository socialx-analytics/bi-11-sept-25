{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzriuQveKwm+F72WtXUbGj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/socialx-analytics/bi-11-sept-25/blob/main/002_scraping_inflasi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "ASkx8k0lnncQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYukreOXmqs1"
      },
      "outputs": [],
      "source": [
        "# Function to scrape BI Rate data\n",
        "def scrape_bi_rate():\n",
        "    # Helper function to extract data from a single page\n",
        "    def scrape_page(soup):\n",
        "        table = soup.find('table', {'class': 'table table-striped table-no-bordered table-lg'})\n",
        "        data = []\n",
        "        for row in table.find_all('tr')[1:]:  # Skip header row\n",
        "            cols = row.find_all('td')\n",
        "            if cols:\n",
        "                no = cols[0].text.strip()\n",
        "                date = cols[1].text.strip()\n",
        "                bi_rate = cols[2].text.strip()\n",
        "                data.append({'No': no, 'Tanggal': date, 'BI-Rate': bi_rate})\n",
        "        return data\n",
        "\n",
        "    url = \"https://www.bi.go.id/id/statistik/indikator/bi-rate.aspx\"\n",
        "    session = requests.Session()\n",
        "    all_data = []\n",
        "\n",
        "    # Loop through 11 pages of data\n",
        "    for page in range(1, 12):\n",
        "        print(f\"Scraping BI-Rate page {page}...\")\n",
        "        if page == 1:\n",
        "            response = session.get(url)\n",
        "        else:\n",
        "            # Prepare payload for POST request to navigate pages\n",
        "            payload = {\n",
        "                \"__EVENTTARGET\": f\"ctl00$ctl54$g_78f62327_0ad4_4bb8_b958_a315eccecc27$ctl00$DataPagerBI7DRR$ctl01$ctl{page-1:02d}\",\n",
        "                \"__EVENTARGUMENT\": \"\",\n",
        "                \"__LASTFOCUS\": \"\",\n",
        "                \"__VIEWSTATE\": soup.find('input', {'name': '__VIEWSTATE'})['value'],\n",
        "                \"__VIEWSTATEGENERATOR\": soup.find('input', {'name': '__VIEWSTATEGENERATOR'})['value'],\n",
        "                \"__EVENTVALIDATION\": soup.find('input', {'name': '__EVENTVALIDATION'})['value'],\n",
        "            }\n",
        "            response = session.post(url, data=payload)\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        page_data = scrape_page(soup)\n",
        "        all_data.extend(page_data)\n",
        "        time.sleep(2)  # Wait between requests to be polite\n",
        "\n",
        "    return pd.DataFrame(all_data)\n",
        "\n",
        "# Function to scrape inflation data\n",
        "def scrape_inflasi():\n",
        "    # Helper function to extract data from a single page\n",
        "    def scrape_page(soup):\n",
        "        table = soup.find('table', {'class': 'table table-striped table-no-bordered table-lg'})\n",
        "        data = []\n",
        "        for row in table.find_all('tr')[1:]:  # Skip header row\n",
        "            cols = row.find_all('td')\n",
        "            if cols:\n",
        "                date = cols[0].text.strip()\n",
        "                inflation = cols[1].text.strip()\n",
        "                data.append({'Tanggal': date, 'Data Inflasi': inflation})\n",
        "        return data\n",
        "\n",
        "    url = \"https://www.bi.go.id/id/statistik/indikator/data-inflasi.aspx\"\n",
        "    session = requests.Session()\n",
        "    all_data = []\n",
        "\n",
        "    # Loop through 26 pages of data\n",
        "    for page in range(1, 27):\n",
        "        print(f\"Scraping Inflation page {page}...\")\n",
        "        if page == 1:\n",
        "            response = session.get(url)\n",
        "        else:\n",
        "            # Prepare payload for POST request to navigate pages\n",
        "            payload = {\n",
        "                \"__EVENTTARGET\": \"ctl00$ctl54$g_1f0a867d_90e9_4a92_b1c8_de34738fc4f1$ctl00$DataPagerDataInflasi$ctl02$ctl00\",\n",
        "                \"__EVENTARGUMENT\": \"\",\n",
        "                \"__LASTFOCUS\": \"\",\n",
        "                \"__VIEWSTATE\": soup.find('input', {'name': '__VIEWSTATE'})['value'],\n",
        "                \"__VIEWSTATEGENERATOR\": soup.find('input', {'name': '__VIEWSTATEGENERATOR'})['value'],\n",
        "                \"__EVENTVALIDATION\": soup.find('input', {'name': '__EVENTVALIDATION'})['value'],\n",
        "            }\n",
        "            response = session.post(url, data=payload)\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        page_data = scrape_page(soup)\n",
        "        all_data.extend(page_data)\n",
        "        time.sleep(2)  # Wait between requests to be polite\n",
        "\n",
        "    return pd.DataFrame(all_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scrape BI Rate data and store in DataFrame\n",
        "df_birate = scrape_bi_rate()\n",
        "df_birate"
      ],
      "metadata": {
        "id": "mViKIA1coqIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scrape inflation data and store in DataFrame\n",
        "df_inflasi = scrape_inflasi()\n",
        "df_inflasi"
      ],
      "metadata": {
        "id": "VpdYKpVunshO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save as CSV\n",
        "df_inflasi.to_csv('inflasi.csv', index=False)\n",
        "df_birate.to_csv('birate.csv', index=False)\n",
        "\n",
        "# Download CSV files\n",
        "files.download('inflasi.csv')\n",
        "files.download('birate.csv')"
      ],
      "metadata": {
        "id": "LGb4kfVEotO5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}